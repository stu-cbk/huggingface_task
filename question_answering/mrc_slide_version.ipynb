{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6b0f28-5529-4620-adb0-3bce26660a85",
   "metadata": {},
   "source": [
    "# 基于滑动窗口策略的机器阅读理解任务实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb1bc8-4943-49bc-9bda-7314d97c11a8",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fadd632-5e62-46fe-9d7a-94b29f917e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e3475d-d795-4fd2-97ea-36e730a9bf5a",
   "metadata": {},
   "source": [
    "## Step2 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51d89bc6-0aec-4f75-9fbc-366fb1a77726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 10142\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 3219\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'answers'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = DatasetDict.load_from_disk(\"mrc_data\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c14ae-b470-4352-a3d0-7566ca4624fd",
   "metadata": {},
   "source": [
    "## Step3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67b328b8-1854-428b-909c-cc0d73f69f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5186ebd0-c9c9-4f3b-96af-1bf7630d1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset = datasets[\"train\"].select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "266a76ad-9a0e-486d-ac2d-beb9e03f18c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_examples = tokenizer(text=simple_dataset[\"question\"],\n",
    "                               text_pair=simple_dataset[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               return_overflowing_tokens=True,\n",
    "                               stride=128,\n",
    "                               max_length=256, truncation=\"only_second\", padding=\"max_length\")\n",
    "tokenized_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bde6a45-6321-4307-9ce6-c2952b05c63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_examples[\"overflow_to_sample_mapping\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83271344-bb66-4f96-9d9e-7dd754de040c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， [SEP]\n",
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 [SEP]\n",
      "[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for sen in tokenizer.batch_decode(tokenized_examples[\"input_ids\"][:3]):\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b165473-fd48-49da-9e8a-bef1b709910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(examples):\n",
    "    tokenized_examples = tokenizer(text=examples[\"question\"],\n",
    "                               text_pair=examples[\"context\"],\n",
    "                               return_offsets_mapping=True,\n",
    "                               return_overflowing_tokens=True,\n",
    "                               stride=128,\n",
    "                               max_length=256, truncation=\"only_second\", padding=\"max_length\")\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    example_ids = []\n",
    "    for idx, _ in enumerate(sample_mapping):\n",
    "        answer = examples[\"answers\"][sample_mapping[idx]]\n",
    "        # [start_char, end_char)\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        # 定位答案在token中的起始位置和结束位置\n",
    "        # 一种策略，我们要拿到context的起始和结束，然后从左右两侧向答案逼近\n",
    "\n",
    "        context_start = tokenized_examples.sequence_ids(idx).index(1)\n",
    "        context_end = tokenized_examples.sequence_ids(idx).index(None, context_start) - 1\n",
    "        offset = tokenized_examples.get(\"offset_mapping\")[idx]\n",
    "        \n",
    "        # 判断答案是否在context中 offset也是[i, i) 左闭右开\n",
    "        if offset[context_end][1] < start_char or offset[context_start][0] > end_char:\n",
    "            start_token_pos = 0\n",
    "            end_token_pos = 0\n",
    "        else:\n",
    "            token_id = context_start\n",
    "            while token_id <= context_end and offset[token_id][0] < start_char:\n",
    "                token_id += 1\n",
    "            start_token_pos = token_id\n",
    "            token_id = context_end\n",
    "            while token_id >= context_start and offset[token_id][1] > end_char:\n",
    "                token_id -=1\n",
    "            end_token_pos = token_id\n",
    "        start_positions.append(start_token_pos)\n",
    "        end_positions.append(end_token_pos)\n",
    "        example_ids.append(examples[\"id\"][sample_mapping[idx]])\n",
    "        # 将其余位置设置为None 辅助预测\n",
    "        tokenized_examples[\"offset_mapping\"][idx] = [\n",
    "            (o if tokenized_examples.sequence_ids(idx)[k] == 1 else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][idx])\n",
    "        ]\n",
    "    tokenized_examples[\"example_ids\"] = example_ids\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86483020-fb68-4439-ae5c-16ed5abaf0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15d9529e3d34f64ba6aa1a0f9a4974e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 36853\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 11967\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_ids', 'start_positions', 'end_positions'],\n",
       "        num_rows: 3776\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenied_datasets = datasets.map(process_func, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
    "tokenied_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbba0ca-09f1-445d-8ab3-ad4bd6274977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'TRAIN_186_QUERY_0': [0, 1, 2, 3, 4, 5],\n",
       "             'TRAIN_186_QUERY_1': [6, 7, 8, 9]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "# example 和 feature的映射\n",
    "example_to_feature = collections.defaultdict(list)\n",
    "for idx, example_id in enumerate(tokenied_datasets[\"train\"][\"example_ids\"][:10]):\n",
    "    example_to_feature[example_id].append(idx)\n",
    "example_to_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e0abf-48ed-4fcf-b9f9-48a7b2899b3a",
   "metadata": {},
   "source": [
    "## Step4 获取模型输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51bab45e-96ef-4026-a6ec-5c94edd60b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# feature 是指tokenized_datasets\n",
    "# examples 是指原始datasets\n",
    "def get_result(start_logits, end_logits, examples, features):\n",
    "    \n",
    "    predictions = {}\n",
    "    references = {}\n",
    "\n",
    "    # example 和 feature的映射\n",
    "    example_to_feature = collections.defaultdict(list)\n",
    "    for idx, example_id in enumerate(features[\"example_ids\"]):\n",
    "        example_to_feature[example_id].append(idx)\n",
    "\n",
    "    # 最优答案候选\n",
    "    n_best = 20\n",
    "    # 最大答案长度\n",
    "    max_answer_length = 30\n",
    "    \n",
    "    for example in examples:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        for feature_idx in example_to_feature[example_id]:\n",
    "            start_logit = start_logits[feature_idx]\n",
    "            end_logit = end_logits[feature_idx]\n",
    "            offset = features[feature_idx][\"offset_mapping\"]\n",
    "            start_indexes = np.argsort(start_logit)[::-1][:n_best].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[::-1][:n_best].tolist()\n",
    "            # start_logits,end_logits内部相互独立\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if offset[start_index] is None or offset[end_index] is None:\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    answers.append({\n",
    "                        \"text\": context[offset[start_index][0]: offset[end_index][1]],\n",
    "                        \"score\": start_logit[start_index] + end_logit[end_index]\n",
    "                    })\n",
    "        if len(answers) > 0:\n",
    "            # 获取可能性最大的start和end组合\n",
    "            best_answer = max(answers, key=lambda x: x[\"score\"])\n",
    "            predictions[example_id] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predictions[example_id] = \"\"\n",
    "        references[example_id] = example[\"answers\"][\"text\"]\n",
    "    \n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d3111c-743f-42e4-9035-c5c78b5846df",
   "metadata": {},
   "source": [
    "## Step5 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68e67f08-dc91-42c5-a97b-6aaf88da0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmrc_eval import evaluate_cmrc\n",
    "\n",
    "def metirc(pred):\n",
    "    # 不需要labels\n",
    "    start_logits, end_logits = pred[0]\n",
    "    if start_logits.shape[0] == len(tokenied_datasets[\"validation\"]):\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"validation\"], tokenied_datasets[\"validation\"])\n",
    "    else:\n",
    "        p, r = get_result(start_logits, end_logits, datasets[\"test\"], tokenied_datasets[\"test\"])\n",
    "    return evaluate_cmrc(p, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df141db-8363-4c93-877e-b2e2903e1791",
   "metadata": {},
   "source": [
    "## Step6 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1bcb66d1-16af-4c44-b930-fc01479eef01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at chinese-macbert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd7a4b-cf84-4215-8dd0-76979be8f598",
   "metadata": {},
   "source": [
    "## Step7 配置TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c81af65a-ffe9-496f-ad36-cfe563a259fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"models_for_qa\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2cafe-8109-428d-9fcf-7deed985fd27",
   "metadata": {},
   "source": [
    "## Step8 配置Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca18b668-fa2e-4806-9c00-29000d9c4e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenied_datasets[\"train\"],\n",
    "    eval_dataset=tokenied_datasets[\"validation\"],\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    compute_metrics=metirc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c5e49-c789-4c41-b547-097fa4834ff7",
   "metadata": {},
   "source": [
    "## Step9 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "253eb954-2e70-4905-b268-e4b71b866c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4607' max='4607' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4607/4607 10:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Avg</th>\n",
       "      <th>F1</th>\n",
       "      <th>Em</th>\n",
       "      <th>Total</th>\n",
       "      <th>Skip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.376900</td>\n",
       "      <td>1.372867</td>\n",
       "      <td>73.697871</td>\n",
       "      <td>83.649237</td>\n",
       "      <td>63.746505</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.187100</td>\n",
       "      <td>1.252813</td>\n",
       "      <td>77.719665</td>\n",
       "      <td>86.722338</td>\n",
       "      <td>68.716993</td>\n",
       "      <td>3219</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4607, training_loss=1.4440986439957648, metrics={'train_runtime': 629.9804, 'train_samples_per_second': 58.499, 'train_steps_per_second': 7.313, 'total_flos': 4814784687995904.0, 'train_loss': 1.4440986439957648, 'epoch': 1.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8839ab-9037-44e1-b71a-f5c90bfcb94c",
   "metadata": {},
   "source": [
    "## Step10 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22404773-4b1f-4470-ae22-cfdd7fdb8203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.question_answering.QuestionAnsweringPipeline at 0x7f1ef95c4490>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "490bab4e-3b3d-4f1e-8ccb-872ea4bc06ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6743096113204956, 'start': 3, 'end': 5, 'answer': '北京'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question=\"小明在哪里上班？\", context=\"小明在北京上班\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76c487-704e-4c83-96a1-a2f00e2f35cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4171241-a9a0-4c28-9072-e81a4b719ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
