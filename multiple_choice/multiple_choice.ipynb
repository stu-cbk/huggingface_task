{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7465fd24-f159-4fa4-a219-3198e59b7be1",
   "metadata": {},
   "source": [
    "# 基于Transformers的多项选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a16f65-a8d5-4dd1-936a-5748f9bc9504",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f629c0-5059-4568-8642-7f6033d60f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96027985-7c8f-4dd0-91b0-da6832272e56",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bf2d26-e8f0-426a-a47a-cbcb6083b057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 1625\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 11869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 3816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3 = DatasetDict.load_from_disk(\"./c3/\")\n",
    "c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00668456-c250-4a95-a75e-b185eae4ba2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'context': ['男：你今天晚上有时间吗?我们一起去看电影吧?', '女：你喜欢恐怖片和爱情片，但是我喜欢喜剧片，科幻片一般。所以……'],\n",
       " 'question': '女的最喜欢哪种电影?',\n",
       " 'choice': ['恐怖片', '爱情片', '喜剧片', '科幻片'],\n",
       " 'answer': '喜剧片'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25b132c-a868-4163-b9a0-8f37f8a51c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "    num_rows: 1625\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310eadc6-b8bf-4e4a-951f-7571bd46181b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 11869\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'context', 'question', 'choice', 'answer'],\n",
       "        num_rows: 3816\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e434f8-0f6e-4956-ae7f-741348c876e3",
   "metadata": {},
   "source": [
    "## Step3 数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72fece1-4908-44f7-9555-b5eeb63e3288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='./chinese-macbert-base', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./chinese-macbert-base\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b4dabb-84ce-4699-b417-d36a79d796f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_function(examples):\n",
    "    # examples, dict, keys: [\"context\", \"quesiton\", \"choice\", \"answer\"]\n",
    "    # examples, 1000\n",
    "    context = []\n",
    "    question_choice = []\n",
    "    labels = []\n",
    "    for idx in range(len(examples[\"context\"])):\n",
    "        ctx = \"\\n\".join(examples[\"context\"][idx])\n",
    "        question = examples[\"question\"][idx]\n",
    "        choices = examples[\"choice\"][idx]\n",
    "        for choice in choices:\n",
    "            context.append(ctx)\n",
    "            question_choice.append(question + \" \" + choice)\n",
    "        if len(choices) < 4:\n",
    "            for _ in range(4 - len(choices)):\n",
    "                context.append(ctx)\n",
    "                question_choice.append(question + \" \" + \"不知道\")\n",
    "        labels.append(choices.index(examples[\"answer\"][idx]))\n",
    "    tokenized_examples = tokenizer(context, question_choice, truncation=\"only_first\", max_length=128, padding=\"max_length\")     # input_ids: 4000 * 256, \n",
    "    tokenized_examples = {k: [v[i: i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}     # 1000 * 4 *256\n",
    "    tokenized_examples[\"labels\"] = labels\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d21169-b017-4b1e-8449-a71a0af255b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00191657ac349768fa8845f9b311477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'context', 'question', 'choice', 'answer', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c3[\"train\"].select(range(10)).map(process_function, batched=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f923645-5d30-4e39-bba9-678324166b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(res[\"input_ids\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1eb170-1595-4386-bc7e-82383cf3d00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe806c165154c5b98ed68b4b6436c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11869 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ac478381ed4fb6a505eb86d3844821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_c3 = c3.map(process_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b762c0b-9e87-4ffe-a22e-a5578a8fc936",
   "metadata": {},
   "source": [
    "## Step4 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17070bc3-145b-46c5-8fcf-9396691261f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at ./chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMultipleChoice.from_pretrained(\"./chinese-macbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839fbb0a-ffd3-4e49-8234-ced08babdaa1",
   "metadata": {},
   "source": [
    "## Step5 创建评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56f08e6f-0b4e-4692-ad37-94c77dc81ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"./accuracy\")\n",
    "\n",
    "def compute_metric(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d9be4-846a-4d00-850b-aa60a55a102b",
   "metadata": {},
   "source": [
    "## Step6 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e52b0fc-b52f-4b48-8f25-e6455b4d04f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./muliple_choice\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c7a7f-7b9e-45a2-afc8-23bd5547124e",
   "metadata": {},
   "source": [
    "## Step7 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f63b3ffa-463a-4d03-be03-c4949b5fc8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_c3[\"train\"],\n",
    "    eval_dataset=tokenized_c3[\"validation\"],\n",
    "    compute_metrics=compute_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d67eb6-e746-4ba0-bcf8-3885eb31f717",
   "metadata": {},
   "source": [
    "## Step8 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7a1b98-0d47-4983-b650-91af908bb360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2968' max='2968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2968/2968 05:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.098400</td>\n",
       "      <td>1.087281</td>\n",
       "      <td>0.513627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2968, training_loss=1.2269923552027289, metrics={'train_runtime': 349.427, 'train_samples_per_second': 33.967, 'train_steps_per_second': 8.494, 'total_flos': 3122837077122048.0, 'train_loss': 1.2269923552027289, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76d432c-078f-4a91-bb46-76c0166899d4",
   "metadata": {},
   "source": [
    "## Step9 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67e3c2f1-848f-4443-bfab-a0c2a0afa0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import torch\n",
    "\n",
    "class MultipleChoicePipeline:\n",
    "    \n",
    "    def __init__(self, model, tokenizer) -> None:\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = model.device\n",
    "    \n",
    "    def preprocess(self, context, question, choices):\n",
    "        cs, qcs = [], []\n",
    "        for choice in choices:\n",
    "            cs.append(context)\n",
    "            qcs.append(question + \" \" + choice)\n",
    "        return tokenizer(cs, qcs, truncation=\"only_first\", max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        inputs = {k : v.unsqueeze(0).to(self.device) for k, v in inputs.items()}\n",
    "        return self.model(**inputs).logits\n",
    "    \n",
    "    def postprocess(self, logits, choices):\n",
    "        prediction = torch.argmax(logits, dim=-1).cpu().item()\n",
    "        return choices[prediction]\n",
    "    \n",
    "    def __call__ (self, context, question, choices) -> Any:\n",
    "        inputs = self.preprocess(context, question, choices)\n",
    "        logits = self.predict(inputs)\n",
    "        result = self.postprocess(logits, choices)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adb54d60-fae6-4ab9-8e0e-7591f36531c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = MultipleChoicePipeline(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2f2cb64-2afb-4e8f-a929-89d7b2cb1c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'北京'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"小明在北京上班\", \"小明在哪里上班？\", [\"北京\", \"上海\", \"河北\", \"海南\", \"河北\", \"海南\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39274f-d221-4869-87d4-ea26e1b0a235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
